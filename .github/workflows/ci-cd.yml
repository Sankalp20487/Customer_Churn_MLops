name: Debug Training Pipeline
  
on: 
  push:
    branches: [ main ]

jobs:
  debug-training:
    runs-on: ubuntu-latest
    env:
      AWS_REGION: us-east-2
      TRAINING_ROLE_ARN: arn:aws:iam::650251710898:role/service-role/AmazonSageMaker-ExecutionRole-20250531T192001
      S3_BUCKET_NAME: customer-churn-project-data
      MLFLOW_TRACKING_URI: arn:aws:sagemaker:us-east-2:650251710898:mlflow-tracking-server/mlflow-ui
      MLFLOW_EXPERIMENT_NAME: ChurnModelTracking
      TRAIN_PARQUET_PATH: s3://customer-churn-project-data/processed/train_cleaned/
      TEST_PARQUET_PATH: s3://customer-churn-project-data/processed/test_cleaned/
      MONITORING_LOGS_DIR: monitoring_logs
      FINAL_MODEL_PATH: models/Best_model.pkl

    steps:
      - name: Check out repo
        uses: actions/checkout@v3

      - name: Set up Python 3.11
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt
          pip install -r modeling/requirements.txt

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      # SKIP GLUE STEPS - using existing processed data

      - name: Run SageMaker training
        run: python scripts/run_training_job.py

      - name: Save best model to S3
        run: python scripts/save_model.py

      - name: Training complete!
        run: echo "ðŸŽ‰ Training and model save completed successfully."